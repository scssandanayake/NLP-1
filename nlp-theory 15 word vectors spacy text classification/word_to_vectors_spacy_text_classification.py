# -*- coding: utf-8 -*-
"""word to vectors spacy text classification.ipynb

Automatically generated by Colab.

## **NLP Tutorial: Text Classification Using Spacy Word Embeddings**

### **Problem Statement**

* Fake news refers to misinformation or disinformation in the country which is spread through word of mouth and more recently through digital communication such as What's app messages, social media posts, etc.

* Fake news spreads faster than real news and creates problems and fear among groups and in society.

* We are going to address these problems using classical NLP techniques and going to classify whether a given message/ text is **Real or Fake Message**.

* We will use **glove embeddings** from spacy which is trained on massive wikipedia dataset to pre-process and text vectorization and apply different classification algorithms.

### **Dataset**

Credits: https://www.kaggle.com/datasets/clmentbisaillon/fake-and-real-news-dataset

* This data consists of two columns. - Text - label

* Text is the statements or messages regarding a particular event/situation.

* label feature tells whether the given text is Fake or Real.

* As there are only 2 classes, this problem comes under the **Binary Classification.**
"""

import pandas as pd
import csv

#read the dataset with name "Fake_Real_Data.csv" and store it in a variable df
df = pd.read_csv("/Fake_Real_Data.csv")

#print the shape of dataframe
print(df.shape)

#print top 5 rows
df.head(5)

#check the distribution of labels
df['label'].value_counts()

"""From the above, we can see that almost the labels(classes) occured equal number of times and balanced. There is no problem of class imbalance and hence no need to apply any balancing techniques like undersampling, oversampling etc."""

#Add the new column which gives a unique number to each of these labels
df['label_num'] = df['label'].map({'Fake' : 0, 'Real': 1})

#check the results with top 5 rows
df.head(5)

"""### **Get spacy word vectors and store them in a pandas dataframe**"""

!python -m spacy download en_core_web_lg # Download the en_core_web_lg model

import spacy

nlp = spacy.load("en_core_web_lg")

"""**Vectorizer EX:-**"""

doc = nlp("NLP and Computer Vision are 2 feilds that are famous in ML domain")
doc.vector

doc.vector.shape

#This will take some time(nearly 15 minutes)
df['vector'] = df['Text'].apply(lambda text: nlp(text).vector)

df.head()

"""A different approach first without vector values to see the diff."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df.vector,
    df.label_num,
    test_size=0.2,
    random_state=2022
)

X_train

"""but we need to get the vector values in it."""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(
    df.vector.values,
    df.label_num,
    test_size=0.2,
    random_state=2022
)

X_train.shape

X_train

X_test.shape

X_test

"""when we look at the actual array of X_train  &  X-test which is the output give as a numpy array , but the every single individual element is another numpy array. when we give this to our classifier for the training it is expecting 2D numpy array. so we need to convert this array from this format to -> a 2D numpy array.

more deatils : https://numpy.org/doc/stable/reference/generated/numpy.stack.html
"""

import numpy as np

X_train_2d = np.stack(X_train)
X_test_2d = np.stack(X_test)
# convert this array from this format to -> a 2D numpy array

X_train_2d

X_test_2d

"""now you see the difference in new arrays

### **Using Naive Bayes Classifier**
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler

clf = MultinomialNB()
clf.fit(X_train_2d, y_train)

from sklearn.naive_bayes import MultinomialNB
from sklearn.preprocessing import MinMaxScaler


scaler = MinMaxScaler()
scaled_train_embed = scaler.fit_transform(X_train_2d)
scaled_test_embed = scaler.transform(X_test_2d)


clf = MultinomialNB()
clf.fit(scaled_train_embed, y_train)

"""negative vector values in array will not accept in MultinomialNB. to tackle this positive negative value issue , we can do Scaling ( it helps to convert the negative values into positife range ) to do is we use MinMaxScaler. we do scalling to both X_train_2D & X_test_2D cause thry both contains negative values. ( X_test_2D will use in evaluation process that why we do scalling for both )"""

from sklearn.metrics import classification_report

y_pred = clf.predict(scaled_test_embed)

print(classification_report(y_test, y_pred))

"""### **Using KNN (K-Nearest neighbors)**"""

from  sklearn.neighbors import KNeighborsClassifier

#1. creating a KNN model object
clf = KNeighborsClassifier(n_neighbors = 5, metric = 'euclidean')

#2. fit with all_train_embeddings and y_train
clf.fit(X_train_2d, y_train)

#3. get the predictions for all_test_embeddings and store it in y_pred
y_pred = clf.predict(X_test_2d)

#4. print the classfication report
print(classification_report(y_test, y_pred))

"""### **Confusion Matrix**"""

#finally print the confusion matrix for the best model
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_test, y_pred)
cm

from matplotlib import pyplot as plt
import seaborn as sn
plt.figure(figsize = (10,7))
sn.heatmap(cm, annot=True, fmt='d')
plt.xlabel('Prediction')
plt.ylabel('Truth')

"""### **Key Takeaways**

1. KNN model which didn't perform well in the vectorization techniques like Bag of words, and TF-IDF due to very high dimensional vector space, performed really well with glove vectors due to only 300-dimensional vectors and very good embeddings(similar and related words have almost similar embeddings) for the given text data.

2. MultinomialNB model performed decently well but did not come into the top list because in the 300-dimensional vectors we also have the negative values present. The Naive Bayes model does not fit the data if there are negative values. So, to overcome this shortcoming, we have used the Min-Max scaler to bring down all the values between 0 to 1. In this process, there will be a possibility of variance and information loss among the data. But anyhow we got a decent recall and f1 scores.
"""