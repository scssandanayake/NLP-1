# -*- coding: utf-8 -*-
"""Stemming and Lemmatization.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WgCeLAv8BWdeCCAFquQOlwQFh472N9Mj

## Stemming in NLTK
"""

import nltk
import spacy

#stemmer type 1
from nltk.stem import PorterStemmer
stemmer = PorterStemmer()

#stemmer type 2
from nltk.stem import SnowballStemmer
stemmer = SnowballStemmer(language='english')  # Specify the language for the stemmer, for example 'english'

words = ["eating", "eats", "eat", "ate", "adjustable", "rafting", "ability", "meeting"]

for word in words:
    print(word, "|", stemmer.stem(word))

"""as you can see stemmers don't use any luienguistic knowledge in stemming process. there are some issues of getting the base word like in ' ate, ability ' likewise.

## Lemmatization in Spacy

spacy don't support stemming
"""

import spacy

nlp = spacy.load("en_core_web_sm")

doc1 = nlp("Mando talked for 3 hours although talking isn't his thing he became talktive")
doc2 = nlp("eating eats eat ate adjustable rafting ability meeting better")
for token in doc1:
    print(token, " | ", token.lemma_)

for token in doc1:
    print(token, " | ", token.lemma_, "|" ,token.lemma)

#genrate the unique identifier of each and every word.
#lemma is printing A hash that unique to each base word that in the english vocabulary on trained model.

for token in doc2:
    print(token, " | ", token.lemma_)

for token in doc2:
    print(token, " | ", token.lemma_, "|" ,token.lemma)

#genrate the unique identifier of each and every word. A hash

"""## Customizing lemmatizer"""

nlp.pipe_names

doc = nlp("Bro, you wanna go? Brah, don't say no! I am exhausted")

for token in doc:
    print(token.text, "|", token.lemma_)

"""the default language model don't understand slangs. but we can customize it.

the 'attribute_ruler' element on this nlp pipe can assign attributes to a particular token.
"""

doc[0]

doc[0].lemma_

doc[6]

doc[6].lemma_

ar = nlp.get_pipe('attribute_ruler')
#this will give that particular 'attribute_ruler' component from the pipeline
#then we can customize it by adding your custom rule

ar.add([[{"TEXT":"Bro"}],[{"TEXT":"Brah"}]],{"LEMMA":"Brother"})

doc = nlp("Bro, you wanna go? Brah, don't say no! I am exhausted")

for token in doc:
    print(token.text, "|", token.lemma_)

doc[0].lemma_ , doc[6].lemma_