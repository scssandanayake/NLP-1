{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming in NLTK"
      ],
      "metadata": {
        "id": "w3floq39nCzV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy"
      ],
      "metadata": {
        "id": "Ik73EMDKwi10"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#stemmer type 1\n",
        "from nltk.stem import PorterStemmer\n",
        "stemmer = PorterStemmer()"
      ],
      "metadata": {
        "id": "Or3nTWGTnHas"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "source": [
        "#stemmer type 2\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer(language='english')  # Specify the language for the stemmer, for example 'english'"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "Qka7fond1yCE"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = [\"eating\", \"eats\", \"eat\", \"ate\", \"adjustable\", \"rafting\", \"ability\", \"meeting\"]\n",
        "\n",
        "for word in words:\n",
        "    print(word, \"|\", stemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IgoShNfIxEvs",
        "outputId": "73635d6c-2492-4e7c-a7d7-780f6beaa7d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating | eat\n",
            "eats | eat\n",
            "eat | eat\n",
            "ate | ate\n",
            "adjustable | adjust\n",
            "rafting | raft\n",
            "ability | abil\n",
            "meeting | meet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "as you can see stemmers don't use any luienguistic knowledge in stemming process. there are some issues of getting the base word like in ' ate, ability ' likewise."
      ],
      "metadata": {
        "id": "v2A6jTtX20ns"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lemmatization in Spacy"
      ],
      "metadata": {
        "id": "1kI8nsIYnHI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "spacy don't support stemming"
      ],
      "metadata": {
        "id": "rzgsh0k84MkE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy"
      ],
      "metadata": {
        "id": "yUL0piKbnKvM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "doc1 = nlp(\"Mando talked for 3 hours although talking isn't his thing he became talktive\")\n",
        "doc2 = nlp(\"eating eats eat ate adjustable rafting ability meeting better\")\n",
        "for token in doc1:\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hnmz-AAG3ZZE",
        "outputId": "55eb7aeb-708e-462f-f606-b1181de96e27"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mando  |  Mando\n",
            "talked  |  talk\n",
            "for  |  for\n",
            "3  |  3\n",
            "hours  |  hour\n",
            "although  |  although\n",
            "talking  |  talk\n",
            "is  |  be\n",
            "n't  |  not\n",
            "his  |  his\n",
            "thing  |  thing\n",
            "he  |  he\n",
            "became  |  become\n",
            "talktive  |  talktive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc1:\n",
        "    print(token, \" | \", token.lemma_, \"|\" ,token.lemma)\n",
        "\n",
        "#genrate the unique identifier of each and every word.\n",
        "#lemma is printing A hash that unique to each base word that in the english vocabulary on trained model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3ea17f8-0d8e-4666-a429-cf293c135c9f",
        "id": "CfdJICQW5dBK"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mando  |  Mando | 7837215228004622142\n",
            "talked  |  talk | 13939146775466599234\n",
            "for  |  for | 16037325823156266367\n",
            "3  |  3 | 602994839685422785\n",
            "hours  |  hour | 9748623380567160636\n",
            "although  |  although | 343236316598008647\n",
            "talking  |  talk | 13939146775466599234\n",
            "is  |  be | 10382539506755952630\n",
            "n't  |  not | 447765159362469301\n",
            "his  |  his | 2661093235354845946\n",
            "thing  |  thing | 2473243759842082748\n",
            "he  |  he | 1655312771067108281\n",
            "became  |  become | 12558846041070486771\n",
            "talktive  |  talktive | 11965990922604149741\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "    print(token, \" | \", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tkXcsK_l4cuE",
        "outputId": "ccf37299-a1b1-4aef-fc06-122b36bccf67"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  |  eat\n",
            "eats  |  eat\n",
            "eat  |  eat\n",
            "ate  |  eat\n",
            "adjustable  |  adjustable\n",
            "rafting  |  raft\n",
            "ability  |  ability\n",
            "meeting  |  meeting\n",
            "better  |  well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for token in doc2:\n",
        "    print(token, \" | \", token.lemma_, \"|\" ,token.lemma)\n",
        "\n",
        "#genrate the unique identifier of each and every word. A hash"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNJuimx_5Dsb",
        "outputId": "84786b5b-e940-4375-f8e7-df00eefad926"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating  |  eat | 9837207709914848172\n",
            "eats  |  eat | 9837207709914848172\n",
            "eat  |  eat | 9837207709914848172\n",
            "ate  |  eat | 9837207709914848172\n",
            "adjustable  |  adjustable | 6033511944150694480\n",
            "rafting  |  raft | 7154368781129989833\n",
            "ability  |  ability | 11565809527369121409\n",
            "meeting  |  meeting | 14798207169164081740\n",
            "better  |  well | 4525988469032889948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Customizing lemmatizer"
      ],
      "metadata": {
        "id": "MmXZC_Dk3dcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp.pipe_names"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cA1mmUfU3fnL",
        "outputId": "044a545e-84ab-4dab-f173-61e73f6adaad"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGov9UT03gxz",
        "outputId": "f96df3d6-b215-407f-a5f5-da9350feca61"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | bro\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brah\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "the default language model don't understand slangs. but we can customize it.\n",
        "\n",
        "the 'attribute_ruler' element on this nlp pipe can assign attributes to a particular token."
      ],
      "metadata": {
        "id": "BwVw2Fyj7d0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QDamUKU3gn8",
        "outputId": "64a21d0f-ff56-428d-dcbc-15f5a19b1d85"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Bro"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uNNW6ASc8Ncs",
        "outputId": "7d95400e-bca7-4c88-9382-2cd42da7af3f"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'bro'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[6]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bo3WWBiB8UFb",
        "outputId": "455f6f01-e26a-4686-84f7-fc7f63acc94d"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Brah"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[6].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "3XZrkrZ68RRs",
        "outputId": "95c26d82-d30a-4314-f933-7acf4a259359"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Brah'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ar = nlp.get_pipe('attribute_ruler')\n",
        "#this will give that particular 'attribute_ruler' component from the pipeline\n",
        "#then we can customize it by adding your custom rule\n",
        "\n",
        "ar.add([[{\"TEXT\":\"Bro\"}],[{\"TEXT\":\"Brah\"}]],{\"LEMMA\":\"Brother\"})\n",
        "\n",
        "doc = nlp(\"Bro, you wanna go? Brah, don't say no! I am exhausted\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token.text, \"|\", token.lemma_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6HJm38j88lM7",
        "outputId": "8239bd25-e091-4ba9-ba4f-f5e139e82cdc"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bro | Brother\n",
            ", | ,\n",
            "you | you\n",
            "wanna | wanna\n",
            "go | go\n",
            "? | ?\n",
            "Brah | Brother\n",
            ", | ,\n",
            "do | do\n",
            "n't | not\n",
            "say | say\n",
            "no | no\n",
            "! | !\n",
            "I | I\n",
            "am | be\n",
            "exhausted | exhaust\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "doc[0].lemma_ , doc[6].lemma_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sufTTYAB8lKL",
        "outputId": "9f64c7d5-48b2-4bca-8649-b7ffee044738"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('Brother', 'Brother')"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    }
  ]
}