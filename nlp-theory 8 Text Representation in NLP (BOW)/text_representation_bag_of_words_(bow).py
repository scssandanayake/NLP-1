# -*- coding: utf-8 -*-
"""Text Representation - Bag Of Words (BOW).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XRNHucIdPpHMT4Oq3qONmZzL-A1gSFE1

## **NLP Tutorial: Text Representation - Bag Of Words (BOW)**
"""

import pandas as pd
import numpy as np

df = pd.read_csv("/spam.csv")
df.head()

df.Category.value_counts()

df['spam']=df['Category'].apply(lambda x:1 if x=='spam' else 0)

"""or else you can use this function to create new spam column"""

def get_spam_number(x):
  if x == 'spam':
    return 1
  else:
    return 0

df.shape

df.head(10)

"""### **Train test split**"""

from sklearn.model_selection import train_test_split

#create training and testing test
X_train, X_test, y_train, y_test = train_test_split(df.Message, df.spam, test_size=0.2)

#x,y independant,dependant variables
#test size 0.2 train size 0.8

"""we use capital X on independant variable (predictor variable", "regressor") cause there can be one or multiple variables/columns. and for the dependant variable (response variable, measured variable) we use simple y cause we predict only one variable using X."""

X_train.shape

X_test.shape

type(X_train)

X_train[:4]

X_train[:][2041]

type(y_train)

y_train[:4]

type(X_train.values)

(X_train.values)

"""## **Create bag of words representation using CountVectorizer**

for more follow the doc: https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html
"""

from sklearn.feature_extraction.text import CountVectorizer

v = CountVectorizer()

X_train_cv = v.fit_transform(X_train.values)
X_train_cv
#this creates a sparse matrix, a big matrix

X_train_cv.toarray()
#converting the sparse matrix to an array
#it generates 2 dimentional numpy arrray

X_train_cv.toarray()[:2]
#view first 2 arrays

X_train_cv.toarray()[:2][0]
#view the first sample only

X_train_cv.toarray()[:1]

X_train_cv.shape
#here we have 7798 unique words in my vocabullary

v.get_feature_names_out().shape

v.get_feature_names_out()
#this gives the entire created vocabullary

v.get_feature_names_out()[10:30]

v.get_feature_names_out()[1000:1050]

dir(v)
#shows all the methods support by this CountVectorizer() variable

v.vocabulary_

v.get_feature_names_out()[7700]

X_train_np_array = X_train_cv.toarray()
X_train_np_array
#converted sparse matrix as an array added to a variable
#easy to view

"""get an idea how this arrays works"""

X_train_np_array[0]

X_train_np_array[:0]

X_train_np_array[:1]

X_train_np_array[1]

X_train_np_array[:6]

X_train_np_array[:6][0]

X_train_np_array[6]

"""done"""

X_train[:4]
#this is the first 4 samples. 1st sample is 2041

X_train[:4][2041]
#1st sample is 2041. view it the email.

np.where(X_train_np_array[0]!=0)
#the which word values becomes 1 on the 1st sample/1st email.
#it means this series gives the word id (unique id creaetd for the position it appears on the voacbullary) that are contain in the sample 1 (1st email)

"""You always make things bigger than they are  ->  [ 954, 1073, 1385, 4392, 6844, 6883, 6889, 7761]

**keep in mind these id are not in order as the sentence word sequence.**
"""

np.where(X_train_np_array[0]==0)
#this contains the word id's for words that are excluded (not within the) from 1st sample

"""#### **let's check it**"""

X_train_np_array[0][954],X_train_np_array[0][1073],X_train_np_array[0][1385],X_train_np_array[0][4392],X_train_np_array[0][6844],X_train_np_array[0][6883],X_train_np_array[0][6889],X_train_np_array[0][7761]

v.get_feature_names_out()[954], v.get_feature_names_out()[1073],v.get_feature_names_out()[1385],v.get_feature_names_out()[4392],v.get_feature_names_out()[6844],v.get_feature_names_out()[6883],v.get_feature_names_out()[6889],v.get_feature_names_out()[7761]

x = [X_train_np_array[0][954],
     X_train_np_array[0][1073],
     X_train_np_array[0][1385],
     X_train_np_array[0][4392],
     X_train_np_array[0][6844],
     X_train_np_array[0][6883],
     X_train_np_array[0][6889],
     X_train_np_array[0][7761]
     ]

x

"""**it checked**

## **Train the naive bayes model**
"""

from sklearn.naive_bayes import MultinomialNB

model = MultinomialNB()
model.fit(X_train_cv, y_train)

X_test_cv = v.transform(X_test)

"""### **Evaluate Performance**"""

from sklearn.metrics import classification_report

y_pred = model.predict(X_test_cv)

print(classification_report(y_test, y_pred))

"""**Hypothesis**

The error "ValueError: X has 3645 features, but MultinomialNB is expecting 7714 features as input." arises because the model was trained on data with 7714 features (likely your X_train_cv), but you are trying to predict on data (X_test_cv) that has only 3645 features.

This discrepancy is usually caused by applying different or incomplete preprocessing steps to the training and testing data. It's likely that the vectorizer (v in your code) used to transform X_test into X_test_cv was either:

**1)Fit on a different dataset:** The vectorizer v might have been fit on a dataset with a different vocabulary or feature set than the one used to train the model.

**2)Partially fit:** If v is a CountVectorizer or TfidfVectorizer, it might have been fit with a limited vocabulary size or other restrictions that resulted in fewer features being extracted from X_test.

**Suggested Changes**

To fix this error, you need to ensure that the X_test_cv data has the same number of features as the data the model was trained on (X_train_cv). This can be achieved by using the same vectorizer instance (with the same vocabulary) that was used during training to transform both the training and testing data.

Here's a potential solution assuming that v is the vectorizer instance:
"""

# Assuming 'v' is the vectorizer instance used during training

# Fit the vectorizer on the training data (if not already done)
# v.fit(X_train)  # Only if 'v' was not already fit during training

# Transform both training and testing data using the same vectorizer instance
X_train_cv = v.transform(X_train)
X_test_cv = v.transform(X_test)

# Train the model
model = MultinomialNB()
model.fit(X_train_cv, y_train)

# Make predictions
y_pred = model.predict(X_test_cv)

# Print the classification report
print(classification_report(y_test, y_pred))

"""Explanation of changes:

**1) Consistent Vectorization:** This code ensures that both the training and testing data are transformed using the same vectorizer instance (v). By doing so, you guarantee that both datasets will have the same features and the model can make predictions without encountering the ValueError.

**2) Fitting the Vectorizer (if needed):** If the vectorizer v wasn't previously fit on your training data during the training phase, you should include the line v.fit(X_train) before transforming the data. This step ensures the vectorizer learns the vocabulary from your training data.

By implementing this solution, you should be able to resolve the ValueError and successfully predict on your test data.

**Sources**

stackoverflow.com/questions/72395184/error-multinomialnb-is-expecting-xxx-features-as-input
365datascience.com/question/valueerror-x-has-1460-features-but-multinomialnb-is-expecting-3925-features-as-input/
stackoverflow.com/questions/74585736/multinomial-naive-bayes-cant-use-validation-dataset-because-of-valueerror-but-c
365datascience.com/question/your-model-doesn-t-appear-to-be-balanced/
scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html
"""

#test the model
emails = [
    'Hey mohan, can we get together to watch footbal game tomorrow?',
    'Upto 20% discount on parking, exclusive offer just for you. Dont miss this reward!'
]

emails_count = v.transform(emails)
model.predict(emails_count)

"""## **Train the model using sklearn pipeline and reduce number of lines of code**"""

from sklearn.pipeline import Pipeline

clf = Pipeline([
    ('vectorizer', CountVectorizer()),
    ('nb', MultinomialNB())
])

clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))