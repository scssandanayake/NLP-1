# -*- coding: utf-8 -*-
"""fastText tutorial .ipynb

Automatically generated by Colab.

## **fastText for NLP tasks**

---

### **Download and explore pre-trained models**
"""

!pip install fasttext

"""## **(1) Explore English Model**

### **Word vectors for 157 languages**

We distribute pre-trained word vectors for 157 languages, trained on Common Crawl and Wikipedia using fastText. These models were trained using CBOW with position-weights, in dimension 300, with character n-grams of length 5, a window of size 5 and 10 negatives. We also distribute three new word analogy datasets, for French, Hindi and Polish.

web: https://fasttext.cc/

get started: https://fasttext.cc/docs/en/support.html#building-fasttext-python-module

installation  guide + model list + model guide: https://fasttext.cc/docs/en/crawl-vectors.html
"""

import fasttext

"""### **Downloading the model ‚ìÇ**

**English word vectors**

This page gathers several pre-trained word vectors trained using fastText.

Download pre-trained word vectors
Pre-trained word vectors learned on different sources can be downloaded below:

1. wiki-news-300d-1M.vec.zip: 1 million word vectors trained on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).
2. wiki-news-300d-1M-subword.vec.zip: 1 million word vectors trained with subword infomation on Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset (16B tokens).
3. crawl-300d-2M.vec.zip: 2 million word vectors trained on Common Crawl (600B tokens).
4. crawl-300d-2M-subword.zip: 2 million word vectors trained with subword information on Common Crawl (600B tokens).

doc: https://fasttext.cc/docs/en/english-vectors.html
"""

import fasttext.util
fasttext.util.download_model('en', if_exists='ignore')  # English
ft = fasttext.load_model('cc.en.300.bin')

# import fasttext
# model_en = fasttext.load_model('C:\\Code\\nlp-tutorials\\downloads\\cc.en.300.bin')

# loading manually downlaoded model

ft.get_nearest_neighbors('good')

#find out nearest simillar words for given token/ word

dir(ft)  #know about available methods

ft.get_word_vector("good")

ft.get_word_vector("good").shape

ft.get_analogies("berlin","germany","france")

#identify relationships between words based on 1st 2 input tokens
#simply analogy is that

ft.get_analogies("berlin","germany","india")

ft.get_analogies("driving","car","phone")

ft.get_analogies("driving","car","book")

ft.get_nearest_neighbors("chutney")

ft.get_nearest_neighbors("kottu")

ft.get_nearest_neighbors("saragva", k=3)

#for some rare words it gives some garbage outputs rarely. out of the datasets.

"""## **(2) Explore Hindi Model**"""

import fasttext.util
fasttext.util.download_model('hi', if_exists='ignore')  # hindi
model_hi = fasttext.load_model('cc.hi.300.bin')

model_hi.get_nearest_neighbors("‡§Ö‡§ö‡•ç‡§õ‡§æ")

model_hi.get_nearest_neighbors("‡§ó‡§æ‡§Ø")

"""
### **Custom train word embeddings on indian food receipes üòã**

dataset credits: https://www.kaggle.com/datasets/sooryaprakash12/cleaned-indian-recipes-dataset"""

import pandas as pd

df = pd.read_csv("/Cleaned_Indian_Food_Dataset.csv")
print(df.shape)
df.head(3)

"""* when we are using CBOW or SkipGram it's an unsupervised approach. it will look scanned through bunch of text and it will create training samples on it's own based on context and target word and then it will train the model."""

df.TranslatedInstructions[0]

import re

text = 'To begin making the Masala Karela Recipe,de-seed the karela and slice.\nDo not remove the skin as the skin has all the nutrients.\nAdd the karela to the pressure cooker with 3 tablespoon of water, salt and turmeric powder and pressure cook for three whistles.\nRelease the pressure immediately and open the lids.\nKeep aside.Heat oil in a heavy bottomed pan or a kadhai.\nAdd cumin seeds and let it sizzle.Once the cumin seeds have sizzled, add onions and saute them till it turns golden brown in color.Add the karela, red chilli powder, amchur powder, coriander powder and besan.\nStir to combine the masalas into the karela.Drizzle a little extra oil on the top and mix again.\nCover the pan and simmer Masala Karela stirring occasionally until everything comes together well.\nTurn off the heat.Transfer Masala Karela into a serving bowl and serve.Serve Masala Karela along with Panchmel Dal and Phulka for a weekday meal with your family.\n'

re.sub(r"[^\w\s]", " ", text, flags=re.MULTILINE)

# remove whitespaces , punctuations and special charachters using RegEx

re.sub(" +"," ","powder  masala powder    coriander")
#if the corpus have more than one spaces ( repititive spaces ) replace that with a singal space

re.sub("[ \n]+"," ","powder  masala powder   \n  coriander")
#remove repititive spaces and \n with single space

def preprocess(text):
    text = re.sub(r'[^\w\s\']',' ', text)
    text = re.sub(r'[ \n]+', ' ', text)
    return text.strip().lower()

# this will remove leading and legging spaces from the text. 'text.strip()'
# convert to smallar case  '.lower()'

text = 'To begin making the Masala Karela Recipe,de-seed the karela and slice.\nDo not remove the skin as the skin has all the nutrients.\nAdd the karela to the pressure cooker with 3 tablespoon of water, salt and turmeric powder and pressure cook for three whistles.\nRelease the pressure immediately and open the lids.\nKeep aside.Heat oil in a heavy bottomed pan or a kadhai.\nAdd cumin seeds and let it sizzle.Once the cumin seeds have sizzled, add onions and saute them till it turns golden brown in color.Add the karela, red chilli powder, amchur powder, coriander powder and besan.\nStir to combine the masalas into the karela.Drizzle a little extra oil on the top and mix again.\nCover the pan and simmer Masala Karela stirring occasionally until everything comes together well.\nTurn off the heat.Transfer Masala Karela into a serving bowl and serve.Serve Masala Karela along with Panchmel Dal and Phulka for a weekday meal with your family.\n'

preprocess(text)

df.TranslatedInstructions = df.TranslatedInstructions.map(preprocess)
#in map function it will map all the entries in that column by created preprocess function.

df.head(3)

df.TranslatedInstructions[0]

"""* the way fastText works is we need have a specific format file whenever we want train the model.

* CBOW or SkipGram are having an unsupervised form of training. we just need the raw text.
"""

df.to_csv("food_receipes.txt", columns=["TranslatedInstructions"], header=None, index=False)

"""### **Train the customized model based on fastText model ( fasttext_model'cc.hi.300.bin ) using our data set**"""

import fasttext

model = fasttext.train_unsupervised("/content/food_receipes.txt")

"""* it is using unsupervised learning approach (CBOW / SkipGram) by default is uses SkipGram.
* it is going through all the text in that file (food_receipes.txt)
* it will create taining paires having context and target words for any pair of words.
* the it train the model. after that it got the word vectors.
"""

model.get_nearest_neighbors("paneer")

model.get_nearest_neighbors("chutney")

model.get_nearest_neighbors("halwa")

model.get_nearest_neighbors("dosa")

model.get_nearest_neighbors("moong")

model.get_word_vector("dosa")

model.get_word_vector("dosa").shape

model.get_nearest_neighbors("saragva")

"""* compare the outputs with early outputs that given by original fastText wiki model

follow official : https://fasttext.cc/docs/en/supervised-tutorial.html

source: https://fasttext.cc/docs/en/unsupervised-tutorial.html

for details on parameters in train_unsupervised function. Based on the need one can use following parameters for fine tunning,

1. epochs = Default value is 5. Epoch is how many times it will loop over the same dataset for the training
2. lr = Learning rate
3. thread = Number of threads for the training
"""